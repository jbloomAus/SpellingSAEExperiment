{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import torch \n",
    "from datasets import load_dataset  \n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import numpy as np\n",
    "import plotly_express as px "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", device = device, dtype = torch.bfloat16)\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-2b-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"blocks.0.hook_resid_post\", # won't always be a hook point\n",
    "    device = device\n",
    ")\n",
    "sae.fold_W_dec_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "n_heads = model.cfg.n_heads\n",
    "d_head = model.cfg.d_head\n",
    "d_mlp = model.cfg.d_mlp\n",
    "d_vocab = model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame(\n",
    "    {\n",
    "        \"token\": np.arange(d_vocab),\n",
    "        \"string\": model.to_str_tokens(np.arange(d_vocab)),\n",
    "    }\n",
    ")\n",
    "vocab_df[\"is_alpha\"] = vocab_df.string.str.match(r\"^( ?)[a-z]+$\")\n",
    "vocab_df[\"is_word\"] = vocab_df.string.str.match(r\"^ [a-z]+$\")\n",
    "vocab_df[\"is_fragment\"] = vocab_df.string.str.match(r\"^[a-z]+$\")\n",
    "vocab_df[\"has_space\"] = vocab_df.string.str.match(r\"^ [A-Za-z]+$\")\n",
    "vocab_df[\"num_chars\"] = vocab_df.string.apply(lambda n: len(n.strip()))\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = [[] for _ in range(20)]\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "for i, row in tqdm(enumerate(vocab_df.iterrows())):\n",
    "    row = row[1]\n",
    "    string = row.string.strip()\n",
    "    for i in range(20):\n",
    "        if not row.is_alpha or i >= len(string):\n",
    "            letters[i].append(-1)\n",
    "        else:\n",
    "            letters[i].append(alphabet.index(string[i]))\n",
    "# %%\n",
    "letters_array = np.array(letters, dtype=np.int32)\n",
    "(letters_array != -1).sum(-1)\n",
    "\n",
    "# %%\n",
    "vocab_df[\"let0\"] = letters_array[0]\n",
    "vocab_df[\"let1\"] = letters_array[1]\n",
    "vocab_df[\"let2\"] = letters_array[2]\n",
    "vocab_df[\"let3\"] = letters_array[3]\n",
    "vocab_df[\"let4\"] = letters_array[4]\n",
    "vocab_df[\"let5\"] = letters_array[5]\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_vocab_df = vocab_df.query(\"is_alpha & num_chars>=4\")\n",
    "sub_vocab_df[\"let0_string\"] = sub_vocab_df.let0.apply(lambda n: alphabet[n] if n != -1 else \"\")\n",
    "print(sub_vocab_df.shape)\n",
    "sub_vocab_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((vocab_df.is_alpha) & (vocab_df.num_chars >= 4)).to_numpy()\n",
    "embed_masked = model.W_E[mask]\n",
    "eff_embed_masked = embed_masked + model.blocks[0].mlp(model.blocks[0].ln2(embed_masked[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_embed_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "char_index = 0\n",
    "col_label = f\"let{char_index}\"\n",
    "X = eff_embed_masked.squeeze().float().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sub_vocab_df[col_label].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "probe = LogisticRegression(max_iter=100)\n",
    "probe.fit(X_train, y_train)\n",
    "probe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test = probe.predict_log_proba(X_test)\n",
    "clp_test = lp_test[np.arange(X_test.shape[0]), y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    x=np.arange(clp_test.size),\n",
    "    y=clp_test,\n",
    "    labels={\"x\": \"sample\", \"y\": \"log prob\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = probe.predict_log_proba(X)\n",
    "clp = lp[np.arange(X.shape[0]), y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_vocab_df[\"correct_class_log_prob\"] = clp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.strip(\n",
    "    sub_vocab_df.sample(2000).reset_index(),\n",
    "    x=\"token\",\n",
    "    y=\"correct_class_log_prob\",\n",
    "    hover_data=[\"string\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_letter_probe_pars = torch.tensor(probe.coef_)\n",
    "first_letter_probe_pars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_feature_virtual_weights = sae.W_dec @ first_letter_probe_pars.T.to(\"cuda\").float()\n",
    "px.line(\n",
    "    probe_feature_virtual_weights.T[1].cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_feature_virtual_weights = sae.W_enc.T @ first_letter_probe_pars.T.to(\"cuda\").float().detach()\n",
    "px.line(\n",
    "    probe_feature_virtual_weights.T[1].cpu().detach().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, inds = torch.topk(probe_feature_virtual_weights.T[1], 5)\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "get_neuronpedia_quick_list(\n",
    "    features=inds.tolist(),\n",
    "    model=\"gemma-2b\",\n",
    "    dataset=\"res-jb\",\n",
    "    layer=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spelling task + attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import circuitsvis as cv\n",
    "\n",
    "def reconstr_hook(activations, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "def zero_abl_hook(mlp_out, hook):\n",
    "    return torch.zeros_like(mlp_out)\n",
    "\n",
    "TEMPLATE = prompt = \"\"\" string: S T R I N G\n",
    " heaven: H E A V E N\n",
    " {}:{}\"\"\"\n",
    "\n",
    "def get_random_word():\n",
    "    return sub_vocab_df.string.sample().values[0].strip()\n",
    "\n",
    "get_random_word()\n",
    "\n",
    "def spell_word(word):\n",
    "    return \" \" + \" \".join(word.upper())\n",
    "\n",
    "spell_word(get_random_word())\n",
    "\n",
    "def get_filled_template(word):\n",
    "    return TEMPLATE.format(word.lower(), spell_word(word))\n",
    "\n",
    "def get_unfilled_template(word):\n",
    "    return TEMPLATE.format(word.lower(), \"\")\n",
    "\n",
    "word = get_random_word()\n",
    "print(get_filled_template(word))\n",
    "\n",
    "\n",
    "print(get_unfilled_template(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import circuitsvis as cv\n",
    "\n",
    "\n",
    "prompt = get_filled_template(get_random_word())\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "display(cv.logits.token_log_probs(model.to_tokens(prompt), model(prompt)[0].log_softmax(dim=-1), model.to_string))\n",
    "sae_out = sae(cache[sae.cfg.hook_name])\n",
    "\n",
    "\n",
    "def reconstr_hook(activations, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "def zero_abl_hook(mlp_out, hook):\n",
    "    return torch.zeros_like(mlp_out)\n",
    "\n",
    "\n",
    "print(\"positive control\")\n",
    "display(cv.logits.token_log_probs(model.to_tokens(prompt), model(prompt)[0].log_softmax(dim=-1), model.to_string))\n",
    "\n",
    "\n",
    "print(\"test group\")\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            sae.cfg.hook_name,\n",
    "            partial(reconstr_hook, sae_out=sae_out),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    display(cv.logits.token_log_probs(model.to_tokens(prompt), model(prompt)[0].log_softmax(dim=-1), model.to_string))\n",
    "\n",
    "\n",
    "print(\"negative control\")\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            sae.cfg.hook_name,\n",
    "            partial(zero_abl_hook),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    display(cv.logits.token_log_probs(model.to_tokens(prompt), model(prompt)[0].log_softmax(dim=-1), model.to_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import circuitsvis as cv\n",
    "\n",
    "\n",
    "prompt = get_filled_template(\"bacon\")\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "display(cv.logits.token_log_probs(model.to_tokens(prompt), model(prompt)[0].log_softmax(dim=-1), model.to_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Based Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional, Callable\n",
    "from transformer_lens import ActivationCache\n",
    "\n",
    "# Metric = Callable[[torch.Tensor, float]]\n",
    "\n",
    "\n",
    "filter_resid_only = lambda name: \"resid\" in name\n",
    "\n",
    "def get_cache_fwd_and_bwd(model, tokens, metric, filter = filter_resid_only):\n",
    "    model.reset_hooks()\n",
    "    cache = {}\n",
    "    def forward_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "    model.add_hook(filter, forward_cache_hook, \"fwd\")\n",
    "\n",
    "    grad_cache = {}\n",
    "    def backward_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "    model.add_hook(filter, backward_cache_hook, \"bwd\")\n",
    "\n",
    "    logits = model(tokens)\n",
    "    value = metric(logits)\n",
    "    value.backward()\n",
    "    model.reset_hooks()\n",
    "    return logits, value.item(), ActivationCache(cache, model), ActivationCache(grad_cache, model)\n",
    "\n",
    "def get_logit_diff_metric(pos_token: str, neg_token: str, model: HookedTransformer):\n",
    "    \n",
    "    def logit_diff_metric(logits: torch.Tensor) -> float:\n",
    "        positive_token_id = model.to_single_token(pos_token)\n",
    "        negative_token_id = model.to_single_token(neg_token)\n",
    "        pos_neg_logit_diff = logits[0,-1,positive_token_id] - logits[0,-1,negative_token_id]\n",
    "        return pos_neg_logit_diff\n",
    "    \n",
    "    return logit_diff_metric\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "clean_tokens = model.to_tokens(prompt)\n",
    "pos_token = \" B\"\n",
    "neg_token = \" A\"\n",
    "logit_diff_metric = get_logit_diff_metric(pos_token, neg_token, model)\n",
    "\n",
    "\n",
    "filter = lambda name: ((\"resid\" in name) or (\"attn\" in name) or (\"mlp\" in name)) and (\"result\" not in name) and (\"_in\" not in name)\n",
    "\n",
    "\n",
    "logits, clean_value, clean_cache, clean_grad_cache = get_cache_fwd_and_bwd(model, clean_tokens, logit_diff_metric, filter)\n",
    "print(\"Clean Value:\", clean_value)\n",
    "print(\"Clean Activations Cached:\", len(clean_cache))\n",
    "print(\"Clean Gradients Cached:\", len(clean_grad_cache))\n",
    "# clean_cache.get_full_resid_decomposition(expand_neurons=False, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit_diff_metric(pos_token: str, neg_token: str, model: HookedTransformer):\n",
    "    \n",
    "    def logit_diff_metric(logits: torch.Tensor) -> float:\n",
    "        positive_token_id = model.to_single_token(pos_token)\n",
    "        negative_token_id = model.to_single_token(neg_token)\n",
    "        pos_neg_logit_diff = logits[0,-1,positive_token_id] - logits[0,-1,negative_token_id]\n",
    "        return pos_neg_logit_diff\n",
    "    \n",
    "    return logit_diff_metric\n",
    "\n",
    "\n",
    "def get_sae_out_all_layers(cache, sae_dict):\n",
    "\n",
    "    sae_outs = {}\n",
    "    feature_actss = {}\n",
    "    for hook_point, sae in sae_dict.items():\n",
    "        feature_acts = sae.encode(cache[hook_point])\n",
    "        sae_out = sae.decode(feature_acts)\n",
    "        sae_outs[hook_point] = sae_out.float()\n",
    "        feature_actss[hook_point] = feature_acts.float()\n",
    "        \n",
    "    return sae_outs, feature_actss\n",
    "\n",
    "saes = {sae.cfg.hook_name: sae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "import re \n",
    "\n",
    "def gradient_based_attributation_all_layers(\n",
    "    model: HookedTransformer,\n",
    "    sparse_autoencoders: dict[str, SAE],\n",
    "    prompt: str  = \"John and Mary went to the store and then John said to\",\n",
    "    metric: Callable[[torch.Tensor], float] = None,\n",
    "    position: int = 2,\n",
    "    test_prompt = False,\n",
    "    ):\n",
    "\n",
    "    if test_prompt:\n",
    "        utils.test_prompt(prompt, pos_token, model, prepend_bos=True)\n",
    "    \n",
    "    logit_diff_metric = get_logit_diff_metric(pos_token, neg_token, model)\n",
    "    logits, clean_value, clean_cache, clean_grad_cache = get_cache_fwd_and_bwd(model, prompt, logit_diff_metric)\n",
    "    sae_outs, feature_actss = get_sae_out_all_layers(cache, sparse_autoencoders)\n",
    "\n",
    "    attribution_dfs = []\n",
    "    for hook_point, sparse_autoencoder in sparse_autoencoders.items():\n",
    "        feature_acts = feature_actss[hook_point]\n",
    "        fired = (feature_acts[0,position,:] > 0).nonzero().squeeze()\n",
    "        activations = feature_acts[0,position,:][fired]\n",
    "        fired_directions = sparse_autoencoder.W_dec[fired]\n",
    "        contributions = activations[:, None] * fired_directions\n",
    "        logit_diff_grad = clean_grad_cache[hook_point][0,position].float()\n",
    "        # attribution_scores = contributions @ pos_neg_logit_diff_direction\n",
    "        attribution_scores = contributions @ logit_diff_grad\n",
    "        \n",
    "        attribution_df = pd.DataFrame(\n",
    "            {\"feature\": fired.detach().cpu().numpy(),\n",
    "            \"activation\": activations.detach().cpu().numpy(),\n",
    "            \"attribution\": attribution_scores.detach().cpu().numpy()})\n",
    "        attribution_df[\"layer\"] = sparse_autoencoder.cfg.hook_name\n",
    "        attribution_df[\"layer_idx\"] = int(re.search(r\"blocks.(\\d+).hook_.*\", sparse_autoencoder.cfg.hook_name).group(1)) + 1*(\"post\" in sparse_autoencoder.cfg.hook_name)\n",
    "        attribution_df[\"position\"] = position\n",
    "        \n",
    "        attribution_dfs.append(attribution_df)\n",
    "        \n",
    "    attribution_df = pd.concat(attribution_dfs)\n",
    "    attribution_df[\"feature\"] = attribution_df.feature.astype(str)\n",
    "    attribution_df[\"layer\"] = attribution_df.layer.astype(\"category\")\n",
    "    \n",
    "    tokens = model.to_str_tokens(prompt)\n",
    "    unique_tokens = [f\"{i}/{tokens[i]}\" for i in range(len(tokens))]\n",
    "    attribution_df[\"unique_token\"]= attribution_df[\"position\"].apply(lambda x: unique_tokens[x])\n",
    "\n",
    "    return attribution_df\n",
    "\n",
    "\n",
    "\n",
    "prompt = get_unfilled_template(\"bacon\")\n",
    "# logits, cache = model.run_with_cache(prompt)\n",
    "# display(cv.logits.token_log_probs(model.to_tokens(prompt), model(prompt)[0].log_softmax(dim=-1), model.to_string))\n",
    "# model.to_string(model.to_tokens(prompt, prepend_bos=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logit_diff_metric = get_logit_diff_metric(pos_token, neg_token, model)\n",
    "\n",
    "attribution_dfs = []\n",
    "n_tokens = len(model.to_str_tokens(prompt))\n",
    "for position in tqdm(range(0,n_tokens)):\n",
    "\n",
    "    attribution_df = gradient_based_attributation_all_layers(\n",
    "        model, saes,\n",
    "        prompt, metric=logit_diff_metric,\n",
    "        position=position,\n",
    "        test_prompt=False)\n",
    "    attribution_dfs.append(attribution_df)\n",
    "\n",
    "attribution_df = pd.concat(attribution_dfs)\n",
    "attribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution_df[attribution_df.position == 19].sort_values(\"attribution\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features= attribution_df[attribution_df.position == 19].sort_values(\"attribution\", ascending=False)[\"feature\"].astype(int).tolist()[:10]\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_neuronpedia_quick_list(\n",
    "    features=[1826, 15367, 2256, 776, 13189, 1357, 5300, 8445, 8918, 8333],\n",
    "    model=\"gemma-2b\",\n",
    "    dataset=\"res-jb\",\n",
    "    layer=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
